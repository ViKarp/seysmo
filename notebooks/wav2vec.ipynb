{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T09:16:36.674137Z",
     "start_time": "2024-09-28T09:16:18.834387Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import segyio\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seysmo.models.model_class import Wav2Vec2Framework, Wav2vec2Loss, ConvFeatureExtractor, ContextualTransformerEncoder\n",
    "from seysmo.models.train_model import train, evaluate\n",
    "from seysmo.models.utils import save_model, load_model, count_parameters, EarlyStopper, SignalSpeedDataset, give_data\n",
    "from seysmo.visualization.plotting import plot_map\n",
    "from seysmo.features.mapping import do_array_for_mapping, compute_y_pred\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ExampleFeatureExtractor(nn.Module):\n",
    "    def __init__(self, extracted_feature_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(27, extracted_feature_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_states = self.linear(inputs)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ExampleEncoder(nn.Module):\n",
    "    def __init__(self, extracted_feature_size, encoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(extracted_feature_size, encoder_hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.linear(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T09:16:36.860869Z",
     "start_time": "2024-09-28T09:16:36.676256Z"
    }
   },
   "id": "15e766ced36f39a6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "tensor(2.3468, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "print(f\"Use device: {device}\")\n",
    "\n",
    "extracted_feature_size = 512\n",
    "encoder_hidden_size = extracted_feature_size\n",
    "\n",
    "feature_extractor = ConvFeatureExtractor().to(device)\n",
    "encoder = ContextualTransformerEncoder(embed_dim=512, num_heads=8, num_layers=12).to(device)\n",
    "\n",
    "# `(batch size, time steps, feature size)`\n",
    "inputs = torch.randn(32, 313, 27).to(device)\n",
    "\n",
    "code_vector_size =  512\n",
    "num_code_vector_groups = 2\n",
    "num_code_vectors_per_group = 100\n",
    "mask_time_prob = 0.02\n",
    "num_mask_time_steps = 3\n",
    "gumbel_init_temperature = 2\n",
    "contrastive_loss_temperature = 0.1\n",
    "num_contrastive_loss_negative_samples = 10\n",
    "loss_alpha = 0.1\n",
    "\n",
    "model = Wav2Vec2Framework(mask_time_prob, num_mask_time_steps, num_code_vector_groups, num_code_vectors_per_group,\n",
    "                 extracted_feature_size, code_vector_size, gumbel_init_temperature, encoder_hidden_size, feature_extractor, encoder).to(device)\n",
    "criterion = Wav2vec2Loss(contrastive_loss_temperature, num_contrastive_loss_negative_samples, num_code_vector_groups,\n",
    "                 num_code_vectors_per_group, loss_alpha)\n",
    "\n",
    "# tuple(Encoder hidden states with mask, Quantized features, Code book perplexity, Time mask indices)\n",
    "model_out = model(inputs)\n",
    "loss = criterion(*model_out)\n",
    "\n",
    "print(loss)\n",
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T09:19:35.925105Z",
     "start_time": "2024-09-28T09:19:33.312642Z"
    }
   },
   "id": "3e2612c2c01bdbe3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 31, 512])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T09:19:37.650403Z",
     "start_time": "2024-09-28T09:19:37.455325Z"
    }
   },
   "id": "9b3a28d5fbcadddc",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 313, 512])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T15:53:51.332502Z",
     "start_time": "2024-09-24T15:53:50.963649Z"
    }
   },
   "id": "4ab2a24e444ac12",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 100])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T15:53:51.819243Z",
     "start_time": "2024-09-24T15:53:51.339993Z"
    }
   },
   "id": "eea102fc89011833",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 313])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out[3].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T15:53:52.145499Z",
     "start_time": "2024-09-24T15:53:51.821601Z"
    }
   },
   "id": "860c2519de18007b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m extracted_feature_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m562\u001B[39m\n\u001B[0;32m      5\u001B[0m encoder_hidden_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[1;32m----> 7\u001B[0m feature_extractor \u001B[38;5;241m=\u001B[39m \u001B[43mExampleFeatureExtractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mextracted_feature_size\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m encoder \u001B[38;5;241m=\u001B[39m ExampleEncoder(extracted_feature_size, encoder_hidden_size)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# `(batch size, time steps, feature size)`\u001B[39;00m\n",
      "File \u001B[1;32m~\\Work\\seysmo\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1170\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1171\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Work\\seysmo\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    778\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 779\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    789\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Work\\seysmo\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 804\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    805\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    807\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\Work\\seysmo\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1153\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1154\u001B[0m             device,\n\u001B[0;32m   1155\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1156\u001B[0m             non_blocking,\n\u001B[0;32m   1157\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1158\u001B[0m         )\n\u001B[1;32m-> 1159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Use device: {device}\")\n",
    "\n",
    "extracted_feature_size = 562\n",
    "encoder_hidden_size = 1024\n",
    "\n",
    "feature_extractor = ExampleFeatureExtractor(extracted_feature_size).to(device)\n",
    "encoder = ExampleEncoder(extracted_feature_size, encoder_hidden_size).to(device)\n",
    "\n",
    "# `(batch size, time steps, feature size)`\n",
    "inputs = torch.randn(4, 313, 27).to(device)\n",
    "# `(batch size)` Number of available time steps per batch\n",
    "input_lengths = torch.tensor([1000, 871, 389, 487]).to(device)\n",
    "\n",
    "\n",
    "model = Wav2Vec2Framework(mask_time_prob, num_mask_time_steps, num_code_vector_groups, num_code_vectors_per_group,\n",
    "                 extracted_feature_size, code_vector_size, gumbel_init_temperature, encoder_hidden_size, feature_extractor, encoder).to(device)\n",
    "criterion = Wav2vec2Loss(contrastive_loss_temperature, num_contrastive_loss_negative_samples, num_code_vector_groups,\n",
    "                 num_code_vectors_per_group, loss_alpha)\n",
    "\n",
    "# tuple(Encoder hidden states with mask, Quantized features, Code book perplexity, Time mask indices)\n",
    "model_out = model(inputs)\n",
    "loss = criterion(*model_out)\n",
    "\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:20:33.118285Z",
     "start_time": "2024-09-24T16:20:31.343180Z"
    }
   },
   "id": "2bfad11094d4a5a4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0cc6594bc9c97e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
